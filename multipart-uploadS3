import requests
import boto3
import math

def upload_large_file_to_s3(url, s3_bucket, s3_key):
    """
    Downloads a file from a URL and uploads it directly to an S3 bucket using multipart upload.
    Ensures that each part uploaded is at least 100 MB to avoid exceeding the 10,000 part limit.
    
    :param url: The URL of the file to download.
    :param s3_bucket: The target S3 bucket.
    :param s3_key: The target S3 key (file path in the bucket).
    :return: Returns the bucket name and key on successful upload.
    """
    MIN_PART_SIZE = 100 * 1024 * 1024  # 100 MB minimum part size
    MAX_PART_SIZE = 5 * 1024 * 1024 * 1024  # 5 GB, S3 maximum part size limit
    s3_client = boto3.client('s3')

    # Start multipart upload
    multipart_upload = s3_client.create_multipart_upload(Bucket=s3_bucket, Key=s3_key)
    upload_id = multipart_upload['UploadId']

    parts = []
    part_number = 1
    bytes_uploaded = 0

    try:
        # Get file size from the URL headers
        response_head = requests.head(url)
        file_size = int(response_head.headers.get('content-length', 0))

        if file_size == 0:
            raise Exception("Failed to determine the file size from the URL.")
        
        # Calculate the optimal part size to avoid exceeding 10,000 parts
        optimal_part_size = max(MIN_PART_SIZE, math.ceil(file_size / 10_000))
        if optimal_part_size > MAX_PART_SIZE:
            raise Exception(f"Calculated part size {optimal_part_size} exceeds S3's 5GB limit.")
        
        print(f"Total file size: {file_size / (1024 * 1024)} MB")
        print(f"Calculated optimal part size: {optimal_part_size / (1024 * 1024)} MB")

        # Step 2: Download the file in parts and upload them to S3
        with requests.get(url, stream=True) as response:
            response.raise_for_status()

            # Download and upload each part
            for chunk in response.iter_content(chunk_size=optimal_part_size):
                if not chunk:
                    break
                
                # Upload the part
                print(f"Uploading part {part_number}...")

                part = s3_client.upload_part(
                    Bucket=s3_bucket,
                    Key=s3_key,
                    PartNumber=part_number,
                    UploadId=upload_id,
                    Body=chunk
                )

                parts.append({'ETag': part['ETag'], 'PartNumber': part_number})
                bytes_uploaded += len(chunk)
                print(f"Uploaded part {part_number}, size: {len(chunk) / (1024 * 1024)} MB")

                part_number += 1

                # Abort if we exceed 10,000 parts
                if part_number > 10_000:
                    raise Exception("Exceeded S3's 10,000 part limit. Aborting.")

        # Step 3: Complete the multipart upload
        s3_client.complete_multipart_upload(
            Bucket=s3_bucket,
            Key=s3_key,
            UploadId=upload_id,
            MultipartUpload={'Parts': parts}
        )
        print(f"File successfully uploaded to s3://{s3_bucket}/{s3_key}")
        return s3_bucket, s3_key

    except Exception as e:
        print(f"Error: {e}")
        if 'upload_id' in locals():
            s3_client.abort_multipart_upload(Bucket=s3_bucket, Key=s3_key, UploadId=upload_id)
            print("Multipart upload aborted due to an error.")
        return None, None

# Example usage
bucket = "your-s3-bucket-name"
key = "path/to/your/file.ova"
url = "https://example.com/path/to/large/ova/file.ova"

uploaded_bucket, uploaded_key = upload_large_file_to_s3(url, bucket, key)

if uploaded_bucket and uploaded_key:
    print(f"File successfully uploaded to s3://{uploaded_bucket}/{uploaded_key}")
else:
    print("Upload failed.")
