import threading
import boto3
import requests
import hashlib
import sys
import tempfile
import os
from boto3.s3.transfer import TransferConfig
from botocore.exceptions import BotoCoreError, ClientError

# Function to download the OVA file to a temporary location and upload it to S3 using multipart upload
def multi_part_upload_with_s3_from_url(bucket_name, s3_key, download_url, min_chunk_size=500 * 1024 * 1024):  # 500 MB chunk size for faster upload
    try:
        # Step 1: Download the file fully into a temporary file
        with tempfile.NamedTemporaryFile(delete=False) as temp_file:
            temp_file_name = temp_file.name
            print(f"Downloading OVA file to temporary location: {temp_file_name}")

            response = requests.get(download_url, stream=True)
            response.raise_for_status()  # Ensure there is no error in downloading the file
            
            total_size = int(response.headers.get('content-length', 0))
            if total_size == 0:
                print("Warning: Couldn't fetch file size from the URL.")
            else:
                print(f"Total file size: {total_size / (1024 * 1024)} MB")
            
            # Write the file to the temp location in chunks
            for chunk in response.iter_content(chunk_size=1024 * 1024):  # 1 MB chunks
                if chunk:
                    temp_file.write(chunk)
            print(f"File download completed. Stored at: {temp_file_name}")

        # Step 2: Upload the file from the temporary location to S3
        s3 = boto3.resource('s3')

        # Calculate chunk size dynamically, ensuring it's at least 500 MB
        max_parts = 10000
        chunk_size = max(min_chunk_size, total_size // max_parts if total_size > 0 else min_chunk_size)  # Dynamic chunk size
        print(f"Chunk size for upload: {chunk_size / (1024 * 1024)} MB")

        config = TransferConfig(
            multipart_threshold=chunk_size,
            multipart_chunksize=chunk_size,
            max_concurrency=10,
            use_threads=True
        )

        # Progress tracking if total_size is known
        callback = ProgressPercentage(total_size) if total_size > 0 else None

        # Upload from file
        print(f"Uploading file to S3 at {s3_key}...")
        with open(temp_file_name, 'rb') as data:
            s3.meta.client.upload_fileobj(
                data,
                bucket_name,
                s3_key,
                Config=config,
                Callback=callback,
                ExtraArgs={'ContentType': 'application/octet-stream'}
            )

        print(f"Upload to S3 completed for {s3_key}.")

        # Step 3: Calculate the SHA256 hash of the original file
        original_hash = calculate_sha256_file_hash(temp_file_name)
        print(f"Original file SHA256 hash: {original_hash}")

        # Step 4: Verify the hash of the uploaded file from S3
        print("Calculating S3 file hash...")
        s3_hash = calculate_s3_sha256_hash(bucket_name, s3_key, chunk_size)
        print(f"S3 file SHA256 hash: {s3_hash}")

        # Compare hashes
        if original_hash == s3_hash:
            print("File integrity check passed: Hashes match.")
        else:
            raise ValueError("File integrity check failed: Hashes do not match.")

        # Return bucket name and s3 key on successful upload
        return bucket_name, s3_key

    except requests.exceptions.RequestException as e:
        print(f"Error during file download: {str(e)}")
    except BotoCoreError as e:
        print(f"Error with AWS SDK or connection: {str(e)}")
    except ClientError as e:
        print(f"S3 Client Error: {str(e)}")
    except ValueError as e:
        print(f"File integrity error: {str(e)}")
    except Exception as e:
        print(f"An unexpected error occurred: {str(e)}")
    finally:
        # Ensure temporary file cleanup
        if os.path.exists(temp_file_name):
            os.remove(temp_file_name)
            print(f"Temporary file {temp_file_name} has been deleted.")
    return None, None

# Function to calculate the SHA256 hash of the local file
def calculate_sha256_file_hash(file_path, chunk_size=1024 * 1024):  # 1 MB chunk size for hashing
    hash_sha256 = hashlib.sha256()
    try:
        with open(file_path, 'rb') as f:
            while True:
                chunk = f.read(chunk_size)
                if not chunk:
                    break
                hash_sha256.update(chunk)
        return hash_sha256.hexdigest()
    except Exception as e:
        print(f"Error calculating file SHA256 hash: {str(e)}")
        raise

# Function to calculate SHA256 hash of an S3 object
def calculate_s3_sha256_hash(bucket_name, s3_key, chunk_size=1024 * 1024):  # Default chunk size for reading S3 object
    s3_client = boto3.client('s3')
    hash_sha256 = hashlib.sha256()
    try:
        # Stream the file from S3
        response = s3_client.get_object(Bucket=bucket_name, Key=s3_key)
        file_stream = response['Body']

        # Read the S3 object in chunks and calculate the hash
        while True:
            chunk = file_stream.read(chunk_size)
            if not chunk:
                break
            hash_sha256.update(chunk)

        return hash_sha256.hexdigest()
    except ClientError as e:
        print(f"S3 Client Error while calculating hash: {str(e)}")
        raise
    except Exception as e:
        print(f"Error calculating S3 file SHA256 hash: {str(e)}")
        raise

# Progress bar class for tracking the upload progress
class ProgressPercentage(object):
    def __init__(self, total_size):
        self._size = float(total_size)
        self._seen_so_far = 0
        self._lock = threading.Lock()

    # Function that gets called as the file uploads
    def __call__(self, bytes_amount):
        with self._lock:
            self._seen_so_far += bytes_amount
            percentage = (self._seen_so_far / self._size) * 100
            sys.stdout.write(
                "\rUploaded %s / %s (%.2f%%)" % (
                    self._seen_so_far, self._size, percentage))
            sys.stdout.flush()

# Example usage
if __name__ == "__main__":
    bucket_name = 'your-s3-bucket-name'
    s3_key = 'path/in/s3/largefile.ova'
    download_url = 'https://example.com/path-to-your-ova-file.ova'

    # Call the function to download and upload the file
    uploaded_bucket, uploaded_key = multi_part_upload_with_s3_from_url(bucket_name, s3_key, download_url)

    if uploaded_bucket and uploaded_key:
        print(f"\nFile successfully uploaded to bucket: {uploaded_bucket}, key: {uploaded_key}")
    else:
        print("\nFile upload failed.")
