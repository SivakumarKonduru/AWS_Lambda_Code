import requests
import boto3
import time
from botocore.exceptions import NoCredentialsError, PartialCredentialsError
from requests.exceptions import RequestException

def stream_to_s3_multipart(url, s3_bucket, s3_key, chunk_size=1024*1024*50, max_retries=5, backoff_factor=0.5):
    """
    Streams a large file from a given URL directly to an S3 bucket using multipart upload with retries and backoff.

    :param url: URL of the file to be streamed and uploaded.
    :param s3_bucket: Name of the S3 bucket.
    :param s3_key: S3 object key (path) where the file will be uploaded.
    :param chunk_size: Size of each chunk to be streamed in bytes. Default is 50MB.
    :param max_retries: Maximum number of retries for failed network requests.
    :param backoff_factor: Time factor for exponential backoff.
    :return: Tuple containing the S3 bucket and key.
    """
    s3_client = boto3.client('s3')

    try:
        # Initiate multipart upload
        multipart_upload = s3_client.create_multipart_upload(Bucket=s3_bucket, Key=s3_key)
        upload_id = multipart_upload['UploadId']
        print(f"Started multipart upload with ID: {upload_id}")
        
        parts = []
        part_number = 1
        bytes_uploaded = 0

        # Open a streaming connection to the URL
        with requests.get(url, stream=True) as response:
            response.raise_for_status()  # Check for HTTP errors

            # We will manually manage chunks
            while True:
                chunk = response.raw.read(chunk_size)  # Manually read the chunk_size in each iteration
                if not chunk:  # No more data
                    break

                retries = 0
                while retries < max_retries:
                    try:
                        print(f"Uploading part {part_number}...")

                        # Upload part to S3
                        part = s3_client.upload_part(
                            Bucket=s3_bucket,
                            Key=s3_key,
                            PartNumber=part_number,
                            UploadId=upload_id,
                            Body=chunk
                        )

                        # Append part information (PartNumber and ETag)
                        parts.append({
                            'ETag': part['ETag'],
                            'PartNumber': part_number
                        })

                        # Increment counters
                        bytes_uploaded += len(chunk)
                        part_number += 1

                        # Break the retry loop once the part is successfully uploaded
                        break

                    except (RequestException, NoCredentialsError, PartialCredentialsError) as e:
                        retries += 1
                        print(f"Error uploading part {part_number}: {e}. Retrying {retries}/{max_retries}...")
                        time.sleep(backoff_factor * (2 ** retries))  # Exponential backoff
                        if retries == max_retries:
                            raise e

        # Complete multipart upload
        s3_client.complete_multipart_upload(
            Bucket=s3_bucket,
            Key=s3_key,
            UploadId=upload_id,
            MultipartUpload={'Parts': parts}
        )
        print(f"Multipart upload completed for {s3_key} with {bytes_uploaded} bytes uploaded.")
        return s3_bucket, s3_key

    except (requests.RequestException, NoCredentialsError, PartialCredentialsError) as e:
        print(f"Error: {e}")
        if 'upload_id' in locals():
            s3_client.abort_multipart_upload(Bucket=s3_bucket, Key=s3_key, UploadId=upload_id)
            print(f"Aborted multipart upload for {s3_key}")
        return None

# Example usage
bucket, key = stream_to_s3_multipart(
    url="https://example.com/path/to/ova/file.ova",  # Replace with the actual URL
    s3_bucket="your-s3-bucket-name",  # Replace with your S3 bucket
    s3_key="uploads/file.ova"  # Replace with your S3 object key (file path in bucket)
)

if bucket and key:
    print(f"File successfully uploaded to s3://{bucket}/{key}")
