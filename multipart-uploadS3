import boto3
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import math

# Initialize the S3 client
s3_client = boto3.client('s3')

# Function to upload large files to S3 using multipart upload
def multipart_upload_from_url_to_s3(bucket_name, s3_key, download_url):
    try:
        # Start the multipart upload
        multipart_upload = s3_client.create_multipart_upload(Bucket=bucket_name, Key=s3_key)
        upload_id = multipart_upload.get('UploadId')

        if upload_id is None:
            raise ValueError("Failed to create multipart upload. No UploadId returned.")

        part_number = 1
        part_info = {'Parts': []}

        # Set up retries for the download request
        retries = Retry(total=5, backoff_factor=0.3, status_forcelist=[500, 502, 503, 504])
        session = requests.Session()
        session.mount('https://', HTTPAdapter(max_retries=retries))

        # Stream the file from the URL without relying on Content-Length
        response = session.get(download_url, stream=True)
        response.raise_for_status()  # Check for download errors

        # Set a default part size (minimum 50MB to avoid too many parts)
        part_size = 50 * 1024 * 1024  # 50 MB

        # Track uploaded parts
        print(f"Starting upload for file {s3_key}...")

        # Read the file in chunks and upload each part to S3
        for chunk in response.iter_content(chunk_size=part_size):
            if chunk:
                try:
                    print(f"Uploading part {part_number}...")

                    # Upload each chunk as a part
                    s3_response = s3_client.upload_part(
                        Body=chunk,
                        Bucket=bucket_name,
                        Key=s3_key,
                        PartNumber=part_number,
                        UploadId=upload_id
                    )

                    etag = s3_response.get('ETag')
                    if etag is None:
                        raise ValueError(f"Failed to upload part {part_number}. No ETag returned.")

                    # Record the part number and ETag for later completion
                    part_info['Parts'].append({
                        'PartNumber': part_number,
                        'ETag': etag
                    })

                    part_number += 1
                except Exception as upload_error:
                    print(f"Error uploading part {part_number}: {upload_error}")
                    raise  # Re-raise the error to trigger multipart abort

        # Complete the multipart upload after all parts are uploaded
        s3_client.complete_multipart_upload(
            Bucket=bucket_name,
            Key=s3_key,
            UploadId=upload_id,
            MultipartUpload=part_info
        )
        print(f"Multipart upload completed successfully for {s3_key}")

        # Return bucket name and s3 key on successful upload
        return bucket_name, s3_key

    except requests.exceptions.RequestException as download_error:
        print(f"Error downloading file: {download_error}")
    except Exception as e:
        print(f"Error during multipart upload: {e}")
        # If there's an error, abort the multipart upload
        if 'UploadId' in locals():
            s3_client.abort_multipart_upload(Bucket=bucket_name, Key=s3_key, UploadId=upload_id)

    # If the upload fails, return None values
    return None, None

# Example usage
bucket_name = "your-s3-bucket-name"
s3_key = "path-in-s3/your-ova-file.ova"  # S3 key (file name in S3)
download_url = "https://example.com/path-to-ova-file.ova"  # URL to download the OVA file

# Call the function and store the returned values
returned_bucket, returned_s3_key = multipart_upload_from_url_to_s3(bucket_name, s3_key, download_url)

# Check if the upload was successful
if returned_bucket and returned_s3_key:
    print(f"File successfully uploaded to bucket: {returned_bucket}, key: {returned_s3_key}")
else:
    print("File upload failed.")
