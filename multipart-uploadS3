import threading
import boto3
import requests
import hashlib
import sys
from boto3.s3.transfer import TransferConfig

# Function to download the OVA file from a URL and upload it directly to S3 using multipart upload
def multi_part_upload_with_s3_from_url(bucket_name, s3_key, download_url):
    # Get the response and total size of the file from the URL
    response = requests.get(download_url, stream=True)
    response.raise_for_status()  # Ensure there is no error in downloading the file

    # Try to get content-length, default to 0 if not available
    total_size = int(response.headers.get('content-length', 0))

    if total_size == 0:
        print("Warning: Couldn't fetch file size from the URL. Progress tracking will be disabled.")

    # Set minimum chunk size to 500 MB
    max_parts = 10000
    min_chunk_size = 500 * 1024 * 1024  # 500 MB in bytes
    chunk_size = max(min_chunk_size, total_size // max_parts if total_size > 0 else min_chunk_size)  # Dynamic chunk size

    # Multipart upload configuration
    config = TransferConfig(
        multipart_threshold=chunk_size,
        multipart_chunksize=chunk_size,
        max_concurrency=10,  # Number of threads to upload parts concurrently
        use_threads=True
    )

    # S3 upload
    s3 = boto3.resource('s3')
    file_obj = response.raw

    # Initialize the hash object for file integrity verification
    sha256_hash = hashlib.sha256()

    # Progress tracking if total_size is known, else disable
    callback = ProgressPercentage(total_size) if total_size > 0 else None

    # Show upload starting message
    print("Uploading in progress...")

    # Perform the upload directly from the response stream without ACL
    s3.meta.client.upload_fileobj(
        file_obj,
        bucket_name,
        s3_key,
        Config=config,
        Callback=callback,  # Only provide callback if total_size is known
        ExtraArgs={'ContentType': 'application/octet-stream'}  # Only setting content type
    )

    # Show upload completed message
    print("\nUpload completed.")

    # Compute the SHA256 hash while downloading the file
    for chunk in response.iter_content(chunk_size=chunk_size):
        sha256_hash.update(chunk)

    # Get the hash digest
    original_hash = sha256_hash.hexdigest()
    print(f"Original file SHA256 hash: {original_hash}")

    # Check the uploaded file hash by downloading it back (not ideal for large files, but for verification)
    uploaded_hash = get_s3_file_hash(bucket_name, s3_key)
    print(f"Uploaded file SHA256 hash: {uploaded_hash}")

    # Compare the original and uploaded hash
    if original_hash == uploaded_hash:
        print("File integrity check passed: hashes match.")
    else:
        print("File integrity check failed: hashes do not match!")

    # Return bucket name and s3 key on successful upload
    return bucket_name, s3_key

# Function to get the hash of the uploaded file in S3
def get_s3_file_hash(bucket_name, s3_key):
    s3_client = boto3.client('s3')
    sha256_hash = hashlib.sha256()

    # Download the file from S3 to compute its hash
    with s3_client.get_object(Bucket=bucket_name, Key=s3_key)['Body'] as s3_object:
        for chunk in iter(lambda: s3_object.read(4096), b""):
            sha256_hash.update(chunk)

    return sha256_hash.hexdigest()

# Progress bar class for tracking the upload progress
class ProgressPercentage(object):
    def __init__(self, total_size):
        self._size = float(total_size)
        self._seen_so_far = 0
        self._lock = threading.Lock()

    # Function that gets called as the file uploads
    def __call__(self, bytes_amount):
        with self._lock:
            self._seen_so_far += bytes_amount
            percentage = (self._seen_so_far / self._size) * 100
            sys.stdout.write(
                f"\rUploaded {self._seen_so_far} / {self._size} (%.2f%%)" % percentage)
            sys.stdout.flush()

# Example usage
if __name__ == "__main__":
    # S3 bucket name and S3 key (file name in S3)
    bucket_name = 'your-s3-bucket-name'
    s3_key = 'path/in/s3/largefile.ova'  # Change this to your desired S3 key

    # OVA file download URL
    download_url = 'https://example.com/path-to-your-ova-file.ova'

    # Call the function to download and upload the file
    uploaded_bucket, uploaded_key = multi_part_upload_with_s3_from_url(bucket_name, s3_key, download_url)

    # Check if the upload was successful
    if uploaded_bucket and uploaded_key:
        print(f"\nFile successfully uploaded to bucket: {uploaded_bucket}, key: {uploaded_key}")
    else:
        print("\nFile upload failed.")
