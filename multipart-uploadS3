import requests
import boto3
import math
import time

def download_and_upload_to_s3(url, s3_bucket, s3_key):
    """
    Downloads a file from a URL and uploads it to an S3 bucket using multipart upload.
    Each part uploaded is guaranteed to be at least 100MB to avoid exceeding the 10,000 part limit.
    
    :param url: URL of the file to download.
    :param s3_bucket: The target S3 bucket name.
    :param s3_key: The key (path) for the file in the S3 bucket.
    :return: Tuple with S3 bucket and key, or None if failed.
    """
    s3_client = boto3.client('s3')
    MIN_PART_SIZE = 100 * 1024 * 1024  # 100 MB minimum part size

    try:
        # Step 1: Start multipart upload
        multipart_upload = s3_client.create_multipart_upload(Bucket=s3_bucket, Key=s3_key)
        upload_id = multipart_upload['UploadId']
        print(f"Multipart upload initiated with ID: {upload_id}")

        parts = []
        part_number = 1
        bytes_uploaded = 0

        # Step 2: Stream download and upload in parts
        with requests.get(url, stream=True, timeout=60) as response:
            response.raise_for_status()

            # Step 3: Iterate over the content in chunks of a fixed size (100 MB)
            for chunk in response.iter_content(chunk_size=MIN_PART_SIZE):
                if not chunk:
                    break

                # Upload each part
                try:
                    print(f"Uploading part {part_number}...")

                    part = s3_client.upload_part(
                        Bucket=s3_bucket,
                        Key=s3_key,
                        PartNumber=part_number,
                        UploadId=upload_id,
                        Body=chunk
                    )

                    parts.append({'ETag': part['ETag'], 'PartNumber': part_number})
                    bytes_uploaded += len(chunk)
                    print(f"Uploaded part {part_number}. Total uploaded: {bytes_uploaded / (1024 * 1024)} MB")

                    part_number += 1

                    # Check if we exceed 10,000 parts
                    if part_number > 10000:
                        raise Exception("Exceeded S3's 10,000 part limit. Aborting upload.")

                except Exception as e:
                    print(f"Error uploading part {part_number}: {e}")
                    s3_client.abort_multipart_upload(Bucket=s3_bucket, Key=s3_key, UploadId=upload_id)
                    return None

        # Step 4: Complete the multipart upload
        s3_client.complete_multipart_upload(
            Bucket=s3_bucket,
            Key=s3_key,
            UploadId=upload_id,
            MultipartUpload={'Parts': parts}
        )
        print(f"Upload completed: s3://{s3_bucket}/{s3_key} Total bytes uploaded: {bytes_uploaded / (1024 * 1024)} MB")
        return s3_bucket, s3_key

    except Exception as e:
        print(f"Error: {e}")
        if 'upload_id' in locals():
            s3_client.abort_multipart_upload(Bucket=s3_bucket, Key=s3_key, UploadId=upload_id)
            print("Multipart upload aborted.")
        return None

# Example usage
bucket, key = download_and_upload_to_s3(
    url="https://example.com/path/to/large/ova/file.ova",  # Replace with the actual URL
    s3_bucket="your-s3-bucket-name",  # Replace with your S3 bucket
    s3_key="uploads/large-file.ova"  # Replace with your S3 object key
)

if bucket and key:
    print(f"File successfully uploaded to s3://{bucket}/{key}")
