import requests
import boto3

def upload_large_file_to_s3(url, s3_bucket, s3_key):
    """
    Downloads a file from a URL and uploads it directly to an S3 bucket using multipart upload.
    It streams the file without requiring the file size to be known upfront.
    
    :param url: The URL of the file to download.
    :param s3_bucket: The target S3 bucket.
    :param s3_key: The target S3 key (file path in the bucket).
    :return: Returns the bucket name and key on successful upload.
    """
    MIN_PART_SIZE = 100 * 1024 * 1024  # 100 MB minimum part size
    s3_client = boto3.client('s3')

    # Start multipart upload
    multipart_upload = s3_client.create_multipart_upload(Bucket=s3_bucket, Key=s3_key)
    upload_id = multipart_upload['UploadId']

    parts = []
    part_number = 1
    bytes_uploaded = 0

    try:
        # Step 1: Stream the file and upload it in parts
        with requests.get(url, stream=True) as response:
            response.raise_for_status()

            def generate_chunks(response, chunk_size=MIN_PART_SIZE):
                """Generator that yields chunks of the response content."""
                for chunk in response.iter_content(chunk_size=chunk_size):
                    if chunk:
                        yield chunk

            for chunk in generate_chunks(response):
                print(f"Uploading part {part_number} with size: {len(chunk) / (1024 * 1024)} MB...")

                part = s3_client.upload_part(
                    Bucket=s3_bucket,
                    Key=s3_key,
                    PartNumber=part_number,
                    UploadId=upload_id,
                    Body=chunk
                )

                parts.append({'ETag': part['ETag'], 'PartNumber': part_number})
                bytes_uploaded += len(chunk)
                part_number += 1

                # Abort if we exceed 10,000 parts
                if part_number > 10_000:
                    raise Exception("Exceeded S3's 10,000 part limit. Aborting.")

        # Step 2: Complete the multipart upload
        s3_client.complete_multipart_upload(
            Bucket=s3_bucket,
            Key=s3_key,
            UploadId=upload_id,
            MultipartUpload={'Parts': parts}
        )
        print(f"File successfully uploaded to s3://{s3_bucket}/{s3_key}")
        return s3_bucket, s3_key

    except Exception as e:
        print(f"Error: {e}")
        if 'upload_id' in locals():
            s3_client.abort_multipart_upload(Bucket=s3_bucket, Key=s3_key, UploadId=upload_id)
            print("Multipart upload aborted due to an error.")
        return None, None

# Example usage
bucket = "your-s3-bucket-name"
key = "path/to/your/file.ova"
url = "https://example.com/path/to/large/ova/file.ova"

uploaded_bucket, uploaded_key = upload_large_file_to_s3(url, bucket, key)

if uploaded_bucket and uploaded_key:
    print(f"File successfully uploaded to s3://{uploaded_bucket}/{uploaded_key}")
else:
    print("Upload failed.")
