import requests
import boto3
import math
import time
from botocore.exceptions import NoCredentialsError, PartialCredentialsError

def download_and_upload_to_s3(url, s3_bucket, s3_key, max_retries=5, backoff_factor=0.5):
    """
    Downloads a large file from a given URL and uploads it directly to an S3 bucket using multipart upload.
    Dynamically calculates chunk sizes to ensure no more than 10,000 parts are used and handles incomplete reads.

    :param url: URL of the file to be streamed and uploaded.
    :param s3_bucket: Name of the S3 bucket.
    :param s3_key: S3 object key (path) where the file will be uploaded.
    :param max_retries: Maximum number of retries for failed network requests.
    :param backoff_factor: Time factor for exponential backoff.
    :return: Tuple containing the S3 bucket and key if successful.
    """
    s3_client = boto3.client('s3')

    try:
        # Attempt to get the file size from the URL
        response = requests.head(url)
        file_size = int(response.headers.get('Content-Length', 0))

        if file_size == 0:
            raise Exception("Failed to determine file size from the URL.")

        print(f"File size: {file_size / (1024 * 1024)} MB")

        # Calculate optimal chunk size to avoid exceeding 10,000 parts
        max_parts = 10_000
        min_part_size = 5 * 1024 * 1024  # 5MB minimum S3 part size
        optimal_chunk_size = max(min_part_size, math.ceil(file_size / max_parts))

        print(f"Using chunk size: {optimal_chunk_size / (1024 * 1024)} MB")

        # Initiate multipart upload
        multipart_upload = s3_client.create_multipart_upload(Bucket=s3_bucket, Key=s3_key)
        upload_id = multipart_upload['UploadId']
        print(f"Started multipart upload with ID: {upload_id}")

        parts = []
        part_number = 1
        bytes_uploaded = 0
        retries = 0

        # Stream the file in chunks
        with requests.get(url, stream=True) as r:
            r.raise_for_status()  # Check for initial request errors

            for chunk in r.iter_content(chunk_size=optimal_chunk_size):
                if chunk:
                    print(f"Uploading part {part_number}...")

                    # Upload part to S3
                    for retry in range(max_retries):
                        try:
                            part = s3_client.upload_part(
                                Bucket=s3_bucket,
                                Key=s3_key,
                                PartNumber=part_number,
                                UploadId=upload_id,
                                Body=chunk
                            )

                            # Append part information (PartNumber and ETag)
                            parts.append({
                                'ETag': part['ETag'],
                                'PartNumber': part_number
                            })

                            # Increment counters
                            bytes_uploaded += len(chunk)
                            part_number += 1

                            if part_number > max_parts:
                                raise Exception("Exceeded S3's 10,000 part limit")

                            # Exit retry loop if successful
                            break

                        except (requests.exceptions.RequestException, NoCredentialsError, PartialCredentialsError) as e:
                            print(f"Error on part {part_number}, retrying: {e}")
                            time.sleep(backoff_factor * (2 ** retry))
                            if retry == max_retries - 1:
                                raise e

        # Complete multipart upload
        s3_client.complete_multipart_upload(
            Bucket=s3_bucket,
            Key=s3_key,
            UploadId=upload_id,
            MultipartUpload={'Parts': parts}
        )

        print(f"Multipart upload completed for {s3_key} with {bytes_uploaded} bytes uploaded.")
        return s3_bucket, s3_key

    except (NoCredentialsError, PartialCredentialsError, Exception) as e:
        print(f"Error: {e}")
        if 'upload_id' in locals():
            s3_client.abort_multipart_upload(Bucket=s3_bucket, Key=s3_key, UploadId=upload_id)
            print(f"Aborted multipart upload for {s3_key}")
        return None


# Example usage
bucket, key = download_and_upload_to_s3(
    url="https://example.com/path/to/large/ova/file.ova",  # Replace with the actual URL
    s3_bucket="your-s3-bucket-name",  # Replace with your S3 bucket
    s3_key="uploads/large-file.ova"  # Replace with your S3 object key (file path in bucket)
)

if bucket and key:
    print(f"File successfully uploaded to s3://{bucket}/{key}")
