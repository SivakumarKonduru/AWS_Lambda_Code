import requests
import boto3
import math

def upload_large_file_to_s3(url, s3_bucket, s3_key):
    """
    Downloads a file from a URL and uploads it directly to an S3 bucket using multipart upload.
    Ensures that each part uploaded is at least 100 MB to avoid exceeding the 10,000 part limit.

    :param url: The URL of the file to download.
    :param s3_bucket: The target S3 bucket.
    :param s3_key: The target S3 key (file path in the bucket).
    """
    MIN_PART_SIZE = 100 * 1024 * 1024  # 100 MB
    s3_client = boto3.client('s3')

    # Start multipart upload
    multipart_upload = s3_client.create_multipart_upload(Bucket=s3_bucket, Key=s3_key)
    upload_id = multipart_upload['UploadId']

    parts = []
    part_number = 1
    bytes_uploaded = 0

    try:
        with requests.get(url, stream=True) as response:
            response.raise_for_status()

            # Use a fixed 100 MB chunk size for each upload part
            for chunk in response.iter_content(chunk_size=MIN_PART_SIZE):
                if not chunk:
                    break
                
                # Upload the part
                print(f"Uploading part {part_number}...")

                part = s3_client.upload_part(
                    Bucket=s3_bucket,
                    Key=s3_key,
                    PartNumber=part_number,
                    UploadId=upload_id,
                    Body=chunk
                )

                parts.append({'ETag': part['ETag'], 'PartNumber': part_number})
                bytes_uploaded += len(chunk)
                print(f"Uploaded part {part_number}, size: {len(chunk) / (1024 * 1024)} MB")

                part_number += 1

                if part_number > 10_000:
                    raise Exception("Exceeded S3's 10,000 part limit!")

        # Complete the multipart upload
        s3_client.complete_multipart_upload(
            Bucket=s3_bucket,
            Key=s3_key,
            UploadId=upload_id,
            MultipartUpload={'Parts': parts}
        )
        print(f"File successfully uploaded to s3://{s3_bucket}/{s3_key}")

    except Exception as e:
        print(f"Error: {e}")
        if 'upload_id' in locals():
            s3_client.abort_multipart_upload(Bucket=s3_bucket, Key=s3_key, UploadId=upload_id)
            print("Multipart upload aborted due to an error.")

# Example usage
bucket = "your-s3-bucket-name"
key = "path/to/your/file.ova"
url = "https://example.com/path/to/large/ova/file.ova"

upload_large_file_to_s3(url, bucket, key)
