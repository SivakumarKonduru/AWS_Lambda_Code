import requests
import boto3
import math
import time
from botocore.exceptions import NoCredentialsError, PartialCredentialsError

def download_and_upload_to_s3(url, s3_bucket, s3_key, default_chunk_size=50 * 1024 * 1024, max_retries=5, backoff_factor=0.5):
    """
    Downloads a file from a URL and uploads it to S3 bucket using multipart upload, without requiring the file size in advance.

    :param url: URL of the file to download.
    :param s3_bucket: The target S3 bucket name.
    :param s3_key: The key (path) for the file in the S3 bucket.
    :param default_chunk_size: Default chunk size (50MB by default).
    :param max_retries: Maximum retries for failed uploads.
    :param backoff_factor: Exponential backoff factor for retries.
    :return: Tuple with S3 bucket and key, or None if failed.
    """
    s3_client = boto3.client('s3')

    try:
        # Start the download stream
        response = requests.get(url, stream=True, timeout=60)
        response.raise_for_status()

        # Start multipart upload on S3
        multipart_upload = s3_client.create_multipart_upload(Bucket=s3_bucket, Key=s3_key)
        upload_id = multipart_upload['UploadId']
        print(f"Multipart upload initiated with ID: {upload_id}")

        parts = []
        part_number = 1
        bytes_uploaded = 0
        retries = 0

        # Stream file in chunks without depending on Content-Length
        for chunk in response.iter_content(chunk_size=default_chunk_size):
            if chunk:  # Filter out keep-alive chunks
                print(f"Uploading part {part_number}...")

                for retry in range(max_retries):
                    try:
                        # Upload the current chunk as a part
                        part = s3_client.upload_part(
                            Bucket=s3_bucket,
                            Key=s3_key,
                            PartNumber=part_number,
                            UploadId=upload_id,
                            Body=chunk
                        )

                        # Append part info to keep track of it for final completion
                        parts.append({
                            'ETag': part['ETag'],
                            'PartNumber': part_number
                        })

                        # Increment part number for the next chunk
                        part_number += 1

                        # Track the total number of bytes uploaded
                        bytes_uploaded += len(chunk)
                        print(f"Uploaded {bytes_uploaded / (1024 * 1024)} MB so far...")

                        # Break retry loop if the upload is successful
                        break

                    except (requests.exceptions.RequestException, NoCredentialsError, PartialCredentialsError) as e:
                        print(f"Error uploading part {part_number}, retrying: {e}")
                        time.sleep(backoff_factor * (2 ** retry))

                        if retry == max_retries - 1:
                            raise e  # Raise the exception if max retries exceeded

                if part_number > 10_000:
                    raise Exception("Exceeded S3's 10,000 part limit")

        # Complete the multipart upload after all parts are uploaded
        s3_client.complete_multipart_upload(
            Bucket=s3_bucket,
            Key=s3_key,
            UploadId=upload_id,
            MultipartUpload={'Parts': parts}
        )
        print(f"Multipart upload completed for {s3_key}. Total bytes uploaded: {bytes_uploaded}.")
        return s3_bucket, s3_key

    except (requests.exceptions.RequestException, NoCredentialsError, PartialCredentialsError, Exception) as e:
        print(f"Error: {e}")
        if 'upload_id' in locals():
            s3_client.abort_multipart_upload(Bucket=s3_bucket, Key=s3_key, UploadId=upload_id)
            print(f"Multipart upload aborted for {s3_key}.")
        return None


# Example usage
bucket, key = download_and_upload_to_s3(
    url="https://example.com/path/to/large/ova/file.ova",  # Replace with the actual URL
    s3_bucket="your-s3-bucket-name",  # Replace with your S3 bucket
    s3_key="uploads/large-file.ova"  # Replace with your S3 object key (file path in bucket)
)

if bucket and key:
    print(f"File successfully uploaded to s3://{bucket}/{key}")
